# This config file is assumed to be parsed pyhocon.

# Training hyperparameters.
max_to_keep = 1         # The number of checkpoints kept.
max_epoch = 50          # The number of epochs in training.
learning_rate = 0.001    # Learning rate.
max_gradient_norm = 5.0 # 
decay_rate = 0.99       #
decay_frequency = 100   #
dropout_rate=0.2        # The dropout ratio in training. 
train_embedding=false   # Whether to retrain the pretrained embeddings or not.
batch_size=128          # Batch size.

# Structure.
num_layers=1                       # The number of layers in MultiRNNCell.
hidden_size=300                    # The dimension of RNN, and other layers.
#rnn_type=bidirectional_dynamic_rnn # The name of rnn function in tensorflow.
cell_type=GRUCell                  # The name of RNNCell class in tensorflow.
attention_type=""
encoder{
  utterance = {
    encoder_type=BidirectionalRNNEncoder
    cell_type=${cell_type}
    output_size=${hidden_size}
    num_layers=${num_layers}
  }
  context = {
    encoder_type=RNNEncoder
    cell_type=${cell_type}
    output_size=${hidden_size}
    num_layers=${num_layers}
  }
}

decoder{
  cell_type=${cell_type}
  hidden_size=${hidden_size}
  num_layers=${num_layers}
  max_len=${utterance_max_len}
  beam_width=5	
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}

model_type=MultiLangDialogueModelWithAutoEncoder

dataset_info={
  dailydialog: ${dailydialog}
  meidai: ${meidai}
}
dataset_path=dataset
dailydialog {
  dataset_type=DailyDialogDataset
  train = {
    path = ${dataset_path}/dailydialog/train.csv
    max_lines = 1000 # The maximum size of training data. if 0, all of the training data will be used.
  }
  valid = {
    path = ${dataset_path}/dailydialog/valid.csv
    max_lines = 100
  }
  test = {
    path = ${dataset_path}/dailydialog/test.csv
    max_lines = 100
  }
}

meidai {
  dataset_type=MeidaiDialogDataset
  train = {
    path = ${dataset_path}/meidai/train.csv
    max_lines = 1000 # The maximum size of training data. if 0, all of the training data will be used.
  }
  valid = {
    path = ${dataset_path}/meidai/valid.csv
    max_lines = 100
  }
  test = {
    path = ${dataset_path}/meidai/test.csv
    max_lines = 100
  }
}


# Text processing.
utterance_max_len = 25
word_max_len = 0
lowercase=true          # Whether to convert words into lowercase or not.
normalize_digits=true
#cbase = true
cbase = false
wbase = true
w_vocab_size = 20000        # The maximum size of the vocabulary. if 0, use all.
c_vocab_size = 100
w_embedding_size = 300
c_embedding_size = 8
task_embedding_size=20
lang_embedding_size=20
feature_size = 10

#Pretrained embeddings.
embedding_path=dataset/embeddings   # The directory where to put your pretrained embeddings file.
embeddings={
  en=${fasttext_300d_en_normed}
  ja=${fasttext_300d_ja_normed}
}

fasttext_300d_en{
  path = ${embedding_path}/fasttext/wiki.en.vec
  skip_first=true
}
fasttext_300d_en_normed{
  path = ${embedding_path}/fasttext/wiki.en.vec.normed.50000
  skip_first=true
}
fasttext_300d_en_mapped{
  path = ${embedding_path}/fasttext/wiki.en.vec.mapped.50000
  skip_first=true
}
fasttext_300d_ja_mapped{
  path = ${embedding_path}/fasttext/wiki.ja.vec.mapped.50000
  skip_first=true
}
fasttext_300d_ja_mapped{
  path = ${embedding_path}/fasttext/wiki.ja.vec.mapped.50000
  skip_first=true
}
fasttext_300d_ja_normed{
  path = ${embedding_path}/fasttext/wiki.ja.vec.normed.50000
  skip_first=true
}
glove_300d_filtered_en {
  path = ${embedding_path}/glove.840B.300d.txt.filtered
  skip_first=false
}
